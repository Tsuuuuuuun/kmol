{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, average_precision_score, accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "from ast import literal_eval\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.rcParams['figure.dpi']=150\n",
    "plt.rcParams['savefig.dpi'] = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chemblv2_500_path = \"/nasa/datasets/kyodai_federated/proj_202111_202203/activity/prepared/chembl_v2_above_500.csv\"\n",
    "splits_path = \"/nasa/datasets/kyodai_federated/proj_202111_202203/activity/prepared/splits.json\"\n",
    "ood_path = \"/nasa/shared_homes/loic/kyodai-kmol/kmol_internal_new/kmol-internal/data/datasets/chemblv2_OOD_set.csv\"\n",
    "superOOD_path = \"/nasa/shared_homes/loic/kyodai-kmol/kmol_internal_new/kmol-internal/data/datasets/chemblv2_superOOD.csv\"\n",
    "\n",
    "\n",
    "original_df = pd.read_csv(chemblv2_500_path)\n",
    "\n",
    "with open(splits_path) as file:\n",
    "    splits = json.load(file)\n",
    "\n",
    "test_df = original_df.iloc[splits['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = \"/nasa/shared_homes/loic/kyodai-kmol/kmol_main2/kmol-internal/data/logs/\"\n",
    "\n",
    "evidential_preds = logs_path+\"bc4_edl_nologits/predictions.csv\"\n",
    "mcdropout_preds = logs_path+\"bc4_mcdropout/predictions.csv\"\n",
    "ensemble_preds = logs_path+\"bc4_ensemble/predictions.csv\"\n",
    "ood_edl_preds = logs_path+\"bc4_edl_nologits_OOD/2022-11-16_09-23/predictions.csv\"\n",
    "superOOD_edl_preds = logs_path+\"bc4_edl_nologits_superOOD/2022-11-16_09-39/predictions.csv\"\n",
    "\n",
    "lrodd_preds = logs_path+\"lrodd_test_full/2023-04-04_14-48/predictions.csv\"\n",
    "ood_lrodd_preds = logs_path+\"lrodd_OOD/2023-04-05_16-00/predictions.csv\"\n",
    "superOOD_lrodd_preds = logs_path+\"lrodd_superOOD/2023-04-05_16-04/predictions.csv\"\n",
    "\n",
    "experiments_dict = {\n",
    "    \"evidential\": {\"preds\": evidential_preds, \"ood_preds\": ood_edl_preds, \"superOOD_preds\": superOOD_edl_preds},\n",
    "    \"mc_dropout\": {\"preds\": mcdropout_preds, \"ood_preds\": None, \"superOOD_preds\": None},\n",
    "    \"ensemble\": {\"preds\": ensemble_preds, \"ood_preds\": None, \"superOOD_preds\": None},\n",
    "    \"LRODD\": {\"preds\": lrodd_preds, \"ood_preds\": ood_lrodd_preds, \"superOOD_preds\": superOOD_lrodd_preds}\n",
    "}\n",
    "\n",
    "#inf_type = \"mc_dropout\"\n",
    "inf_type = \"LRODD\"\n",
    "#inf_type = \"ensemble\"\n",
    "#inf_type = \"LROOD\"\n",
    "ood_tests = False\n",
    "\n",
    "# Creating the predictions dataframe\n",
    "predictions = pd.read_csv(experiments_dict[inf_type][\"preds\"])\n",
    "predictions.set_index('id', inplace=True)\n",
    "\n",
    "test_df = test_df.loc[predictions.index]\n",
    "\n",
    "def read_and_set_index(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.set_index('id', inplace=True)\n",
    "    return df\n",
    "\n",
    "if experiments_dict[inf_type][\"ood_preds\"] != None:\n",
    "    ood_tests = True\n",
    "    predictions_ood = read_and_set_index(experiments_dict[inf_type][\"ood_preds\"])\n",
    "    ood_df = pd.read_csv(ood_path)\n",
    "    ood_df = ood_df.loc[predictions_ood.index]\n",
    "    predictions_ood[\"target\"] = ood_df[\"target_sequence\"]\n",
    "\n",
    "    predictions_superOOD = read_and_set_index(experiments_dict[inf_type][\"superOOD_preds\"])\n",
    "    superOOD_df = pd.read_csv(superOOD_path)\n",
    "    superOOD_df = superOOD_df.loc[predictions_superOOD.index]\n",
    "    predictions_superOOD[\"target\"] = superOOD_df[\"target_sequence\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(preds_df, inf_type, threshold=0.5):\n",
    "    if inf_type == \"evidential\":\n",
    "        preds_df[\"t_100n_error\"] = abs((preds_df[\"t_100n_ground_truth\"] == preds_df[\"t_100n\"]).astype(int)-preds_df[\"t_100n_softmax\"])\n",
    "        preds_df[\"t_1u_error\"] = abs((preds_df[\"t_1u_ground_truth\"] == preds_df[\"t_1u\"]).astype(int)-preds_df[\"t_1u_softmax\"])\n",
    "        preds_df[\"t_10u_error\"] = abs((preds_df[\"t_10u_ground_truth\"] == preds_df[\"t_10u\"]).astype(int)-preds_df[\"t_10u_softmax\"])\n",
    "\n",
    "        preds_df[\"t_100n_logits_aleatoric\"] = abs(1 - preds_df[\"t_100n_softmax\"])\n",
    "        preds_df[\"t_1u_logits_aleatoric\"] = abs(1 - preds_df[\"t_1u_softmax\"])\n",
    "        preds_df[\"t_10u_logits_aleatoric\"] = abs(1 - preds_df[\"t_10u_softmax\"])\n",
    "    else:\n",
    "        preds_df[\"t_100n_error\"] = abs(preds_df[\"t_100n_ground_truth\"] - preds_df[\"t_100n\"])\n",
    "        preds_df[\"t_1u_error\"] = abs(preds_df[\"t_1u_ground_truth\"] - preds_df[\"t_1u\"])\n",
    "        preds_df[\"t_10u_error\"] = abs(preds_df[\"t_10u_ground_truth\"] - preds_df[\"t_10u\"])\n",
    "\n",
    "        preds_df[\"t_100n_logits_aleatoric\"] = abs(preds_df[\"t_100n\"] - (preds_df[\"t_100n\"] > threshold).astype(int))\n",
    "        preds_df[\"t_1u_logits_aleatoric\"] = abs(preds_df[\"t_1u\"] - (preds_df[\"t_1u\"] > threshold).astype(int))\n",
    "        preds_df[\"t_10u_logits_aleatoric\"] = abs(preds_df[\"t_10u\"] - (preds_df[\"t_10u\"] > threshold).astype(int))\n",
    "\n",
    "    preds_df[\"t_100n_error_thresh\"] = abs(preds_df[\"t_100n_ground_truth\"] - (preds_df[\"t_100n\"] > threshold).astype(int))\n",
    "    preds_df[\"t_1u_error_thresh\"] = abs(preds_df[\"t_1u_ground_truth\"] - (preds_df[\"t_1u\"] > threshold).astype(int))\n",
    "    preds_df[\"t_10u_error_thresh\"] = abs(preds_df[\"t_10u_ground_truth\"] - (preds_df[\"t_10u\"] > threshold).astype(int))\n",
    "\n",
    "    preds_df[\"cumulated_error\"] = preds_df[\"t_100n_error\"] + preds_df[\"t_1u_error\"] + preds_df[\"t_10u_error\"]\n",
    "    preds_df[\"cumulated_error_thresh\"] = preds_df[\"t_100n_error_thresh\"] + preds_df[\"t_1u_error_thresh\"] + preds_df[\"t_10u_error_thresh\"]\n",
    "    if \"likelihood_ratio\" in preds_df.columns:\n",
    "        preds_df[\"t_100n_logits_var\"], preds_df[\"t_1u_logits_var\"], preds_df[\"t_10u_logits_var\"] = preds_df[\"likelihood_ratio\"], preds_df[\"likelihood_ratio\"], preds_df[\"likelihood_ratio\"]\n",
    "        preds_df[\"cumulated_uncertainty\"] = preds_df[\"likelihood_ratio\"]\n",
    "    else:\n",
    "        preds_df[\"cumulated_uncertainty\"] = preds_df[\"t_100n_logits_var\"] + preds_df[\"t_1u_logits_var\"] + preds_df[\"t_10u_logits_var\"]\n",
    "    preds_df[\"cumulated_uncertainty_aleatoric\"] = preds_df[\"t_100n_logits_aleatoric\"] + preds_df[\"t_1u_logits_aleatoric\"] + preds_df[\"t_10u_logits_aleatoric\"]\n",
    "\n",
    "    return preds_df\n",
    "\n",
    "predictions = process_predictions(predictions, inf_type)\n",
    "\n",
    "if experiments_dict[inf_type][\"ood_preds\"] != None:\n",
    "    predictions_ood = process_predictions(predictions_ood, inf_type)\n",
    "    predictions_superOOD = process_predictions(predictions_superOOD, inf_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unc_to_error(preds, title='uncertainty to error', save_path=\"./mt_unc_to_error.png\", error_col=\"cumulated_error_thresh\", unc_col=\"cumulated_uncertainty\", save=False, preds_ood=[], preds_superOOD=[]):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    # Get the error and uncertainty values from the preds dataframe\n",
    "    error = preds[error_col].values\n",
    "    uncertainty = preds[unc_col].values # .abs()\n",
    "\n",
    "    # size of points in plots\n",
    "    point_size = 1\n",
    "\n",
    "    # Plot the points for the preds dataframe\n",
    "    ax.scatter(uncertainty, error, s=point_size)\n",
    "\n",
    "    # Check if there are OOD or superOOD preds dataframes\n",
    "    if len(preds_ood) > 0:\n",
    "        # Get the error and uncertainty values from the OOD preds dataframe\n",
    "        error_ood = preds_ood[error_col].values\n",
    "        uncertainty_ood = preds_ood[unc_col].values #abs\n",
    "\n",
    "        # Get the error and uncertainty values from the superOOD preds dataframe\n",
    "        error_superOOD = preds_superOOD[error_col].values\n",
    "        uncertainty_superOOD = preds_superOOD[unc_col].values #abs\n",
    "\n",
    "        # Plot the points for the OOD and superOOD preds dataframes\n",
    "        ax.scatter(uncertainty_ood, error_ood, s=point_size, c=\"green\", label=\"OOD\")\n",
    "        ax.scatter(uncertainty_superOOD, error_superOOD, s=point_size, c=\"red\", label=\"superOOD\")\n",
    "    \n",
    "    # Set the plot title, x-axis label, and y-axis label\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('likelihood ratio')\n",
    "    ax.set_ylabel('error')\n",
    "    \n",
    "    # Save the plot if specified\n",
    "    if save:\n",
    "        fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_unc_to_error(predictions, save_path=\"./mt_unc_to_error_thresh_(\"+inf_type+\").png\")\n",
    "plot_unc_to_error(predictions, error_col=\"t_100n_error_thresh\")\n",
    "plot_unc_to_error(predictions, error_col=\"cumulated_error_thresh\", unc_col=\"cumulated_uncertainty\")\n",
    "\n",
    "\"\"\"\n",
    "if ood_tests:\n",
    "    plot_unc_to_error(predictions, save_path=\"./mt_unc_to_error_oods_(\"+inf_type+\").png\",thresh=False, preds_ood=predictions_ood, preds_superOOD=predictions_superOOD, save=True)\n",
    "    plot_unc_to_error(predictions, save_path=\"./mt_unc_to_error_thresh_oods_(\"+inf_type+\").png\",thresh=True, preds_ood=predictions_ood, preds_superOOD=predictions_superOOD, save=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"protein_id\"] = test_df[\"target_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_prot_avg_df = predictions.groupby([\"protein_id\"]).mean()\n",
    "if ood_tests:\n",
    "    per_prot_avg_ood_df = predictions_ood.groupby([\"target\"]).mean()\n",
    "    per_prot_avg_superOOD_df = predictions_superOOD.groupby([\"target\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384798 75703\n",
      "Best threshold:  3.5762784e-08\n",
      "Best F1-Score:  nan\n",
      "auprc 0.749\n",
      "accuracy norm - ood - superOOD 0.887 0.739 0.705\n",
      "error_avg norm - ood - superOOD 0.339 0.784 0.885\n",
      "unc_avg norm - ood - superOOD -0.0 -0.0 -0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a2669cf128c0>:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if ood_tests:\n",
    "    accuracy = 1-(predictions[\"cumulated_error_thresh\"].mean()/3.)\n",
    "    accuracy_ood = 1-(predictions_ood[\"cumulated_error_thresh\"].mean()/3.)\n",
    "    accuracy_superOOD = 1-(predictions_superOOD[\"cumulated_error_thresh\"].mean()/3.)\n",
    "\n",
    "    avg_error = predictions[\"cumulated_error_thresh\"].mean()\n",
    "    avg_error_ood = predictions_ood[\"cumulated_error_thresh\"].mean()\n",
    "    avg_error_superOOD = predictions_superOOD[\"cumulated_error_thresh\"].mean()\n",
    "\n",
    "    avg_unc = predictions[\"cumulated_uncertainty\"].mean()\n",
    "    avg_unc_ood = predictions_ood[\"cumulated_uncertainty\"].mean()\n",
    "    avg_unc_superOOD = predictions_superOOD[\"cumulated_uncertainty\"].mean()\n",
    "\n",
    "    from sklearn.metrics import precision_recall_curve, auc\n",
    "    normal_scores = predictions[\"cumulated_uncertainty\"].tolist()\n",
    "    ood_scores = predictions_ood[\"cumulated_uncertainty\"].tolist() + predictions_superOOD[\"cumulated_uncertainty\"].tolist()\n",
    "    y_scores = normal_scores + ood_scores\n",
    "    y_norm = [0] * len(normal_scores)\n",
    "    y_ood = [1] * len(ood_scores)\n",
    "    y_labels = y_norm+y_ood\n",
    "    print(len(y_ood), len(y_norm))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_labels, y_scores)\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    f1_scores = 2*recall*precision/(recall+precision)\n",
    "    print('Best threshold: ', thresholds[np.argmax(f1_scores)])\n",
    "    print('Best F1-Score: ', np.max(f1_scores))\n",
    "    print('auprc', round(auc_precision_recall, 3))\n",
    "\n",
    "print(\"accuracy norm - ood - superOOD\", round(accuracy, 3), round(accuracy_ood, 3), round(accuracy_superOOD, 3))\n",
    "print(\"error_avg norm - ood - superOOD\", round(avg_error, 3), round(avg_error_ood, 3), round(avg_error_superOOD, 3))\n",
    "print(\"unc_avg norm - ood - superOOD\", round(avg_unc, 3), round(avg_unc_ood,3), round(avg_unc_superOOD,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot_unc_to_error(per_prot_avg_df, title='Scatter plot uncertainty to error avg per protein', save_path='./mt_unc_to_error_avg_prot_('+inf_type+').png')\n",
    "#plot_unc_to_error(per_prot_avg_df, title='Scatter plot uncertainty to error avg per protein', save_path='./mt_unc_to_error_avg_prot_thresh_('+inf_type+').png')\n",
    "#plot_unc_to_error(per_prot_avg_df, title='Scatter plot uncertainty to error avg per protein', save_path='./mt_unc_to_error_avg_prot_thresh_aloatoric('+inf_type+').png', aleatoric=True)\n",
    "\n",
    "if ood_tests:\n",
    "    #plot_unc_to_error(per_prot_avg_df, title='Scatter plot uncertainty to error avg per protein', save_path='./mt_unc_to_error_avg_prot_thresh_ood_('+inf_type+').png', preds_ood=per_prot_avg_ood_df, preds_superOOD=per_prot_avg_superOOD_df, save=True)\n",
    "    for col in [\"t_100n\", \"t_1u\", \"t_10u\"]:\n",
    "        plot_unc_to_error(per_prot_avg_df, title='likelihood ratio to error per protein with OOD samples at threshold '+col, save_path='./mt_unc_to_error_avg_prot_thresh_ood_('+inf_type+col+').png', error_col=col+\"_error_thresh\", unc_col=col+\"_logits_var\", preds_ood=per_prot_avg_ood_df, preds_superOOD=per_prot_avg_superOOD_df, save=True)\n",
    "    plot_unc_to_error(per_prot_avg_df, title='likelihood ratio to error per protein with OOD samples for cumulated thresholds', save_path='./mt_unc_to_error_avg_prot_thresh_ood_('+inf_type+').png', preds_ood=per_prot_avg_ood_df, preds_superOOD=per_prot_avg_superOOD_df, save=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_density(preds, title='likehood ratio density', save_path=\"./likehood_ratio_density.png\", unc_col=\"likelihood_ratio\", save=False, preds_ood=[], preds_superOOD=[]):\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    # Get the error and uncertainty values from the preds dataframe\n",
    "    uncertainty = preds[unc_col].values # .abs()\n",
    "\n",
    "    # Plot the distribution of uncertainties for normal samples\n",
    "    sns.distplot(uncertainty, ax=ax, color=\"blue\", bins=200, hist_kws=dict(edgecolor=\"k\", linewidth=1, alpha=0.5), kde_kws={\"color\": \"blue\", \"lw\": 4, \"label\": \"Normal\"})\n",
    "\n",
    "    # Check if there are OOD or superOOD preds dataframes\n",
    "    if len(preds_ood) > 0:\n",
    "        # Get the error and uncertainty values from the OOD preds dataframe\n",
    "        uncertainty_ood = preds_ood[unc_col].values #abs\n",
    "\n",
    "        # Get the error and uncertainty values from the superOOD preds dataframe\n",
    "        uncertainty_superOOD = preds_superOOD[unc_col].values #abs\n",
    "\n",
    "         # Plot the distribution of uncertainties for OOD and superOOD samples\n",
    "        sns.distplot(uncertainty_superOOD, ax=ax, color=\"red\", bins=200, hist_kws=dict(edgecolor=\"k\", linewidth=1, alpha=0.5), kde_kws={\"color\": \"red\", \"lw\": 2, \"label\": \"superOOD\"})\n",
    "        sns.distplot(uncertainty_ood, ax=ax, color=\"green\", bins=200, hist_kws=dict(edgecolor=\"k\", linewidth=1, alpha=0.5), kde_kws={\"color\": \"green\", \"lw\": 2, \"label\": \"OOD\"})\n",
    "       \n",
    "    \n",
    "    # Set the plot title, x-axis label, and y-axis label\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('likelihood ratio')\n",
    "    ax.set_ylabel('density')\n",
    "\n",
    "    # Set logarithmic scale for the y-axis\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_ylim(top=100)\n",
    "    \n",
    "    # Save the plot if specified\n",
    "    if save:\n",
    "        fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ood_tests:\n",
    "     plot_lr_density(per_prot_avg_df, preds_ood=per_prot_avg_ood_df, preds_superOOD=per_prot_avg_superOOD_df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_unc_to_error(per_prot_avg_df, title='Scatter plot uncertainty to error avg per protein', save_path='./mt_unc_to_error_avg_prot_thresh_ood_('+inf_type+').png', thresh=True, preds_ood=per_prot_avg_ood_df, preds_superOOD=per_prot_avg_superOOD_df, save=False, aleatoric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_df = pd.read_csv('/nasa/datasets/kyodai_federated/proj_202111_202203/activity/raw/protein-classification-all.csv')\n",
    "prot_df = prot_df.loc[:, ['pref_name', 'short_name', 'sequence']]\n",
    "\n",
    "prot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sequences = []\n",
    "for id in per_prot_avg_df.index:\n",
    "    row = test_df[test_df[\"target_id\"] == id]\n",
    "    target_sequence = row[\"target_sequence\"].iloc[0]\n",
    "    target_sequences.append(target_sequence)\n",
    "\n",
    "per_prot_avg_df[\"target_sequence\"] = target_sequences\n",
    "\n",
    "per_prot_avg_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_names = []\n",
    "short_names = []\n",
    "\n",
    "for id in per_prot_avg_df.index:\n",
    "    sequence = per_prot_avg_df.loc[id][\"target_sequence\"]\n",
    "    try:\n",
    "        sequence_info = prot_df[prot_df[\"sequence\"] == sequence].iloc[0]\n",
    "    except IndexError:\n",
    "        sequence_info = {\"pref_name\": \"unknown\", \"short_name\":\"unknown\"}\n",
    "    pref_names.append(sequence_info[\"pref_name\"])\n",
    "    short_names.append(sequence_info[\"short_name\"])\n",
    "\n",
    "\n",
    "per_prot_avg_df[\"pref_name\"] = pref_names\n",
    "per_prot_avg_df[\"short_name\"] = pref_names\n",
    "\n",
    "\n",
    "per_prot_avg_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_prot_families_avg_df = per_prot_avg_df.groupby([\"short_name\"]).mean()\n",
    "per_prot_families_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unc_to_error(per_prot_families_avg_df, title='Scatter plot uncertainty to error avg per prot families', save_path='./mt_unc_to_error_avg_prot_families_('+inf_type+').png')\n",
    "plot_unc_to_error(per_prot_families_avg_df, title='Scatter plot uncertainty to error avg per prot families', save_path='./mt_unc_to_error_avg_prot_families_thersh_('+inf_type+').png', thresh=True)\n",
    "plot_unc_to_error(per_prot_families_avg_df, title='Scatter plot uncertainty to error avg per prot families', save_path='./mt_unc_to_error_avg_prot_families_thresh_aleatoric_('+inf_type+').png', thresh=True, aleatoric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_per_sample = predictions[\"cumulated_uncertainty\"].corr(predictions[\"cumulated_error\"])\n",
    "R_per_protein = per_prot_avg_df[\"cumulated_uncertainty\"].corr(per_prot_avg_df[\"cumulated_error\"])\n",
    "R_per_protein_families = per_prot_families_avg_df[\"cumulated_uncertainty\"].corr(per_prot_families_avg_df[\"cumulated_error\"])\n",
    "\n",
    "R_per_sample_thresh = predictions[\"cumulated_uncertainty\"].corr(predictions[\"cumulated_error_thresh\"])\n",
    "R_per_protein_thresh = per_prot_avg_df[\"cumulated_uncertainty\"].corr(per_prot_avg_df[\"cumulated_error_thresh\"])\n",
    "R_per_protein_families_thresh = per_prot_families_avg_df[\"cumulated_uncertainty\"].corr(per_prot_families_avg_df[\"cumulated_error_thresh\"])\n",
    "\n",
    "R_per_sample_thresh_aleatoric = predictions[\"cumulated_uncertainty_aleatoric\"].corr(predictions[\"cumulated_error_thresh\"])\n",
    "R_per_protein_thresh_aleatoric = per_prot_avg_df[\"cumulated_uncertainty_aleatoric\"].corr(per_prot_avg_df[\"cumulated_error_thresh\"])\n",
    "R_per_protein_families_thresh_aleatoric = per_prot_families_avg_df[\"cumulated_uncertainty_aleatoric\"].corr(per_prot_families_avg_df[\"cumulated_error_thresh\"])\n",
    "\n",
    "accuracy = 1-(predictions[\"cumulated_error_thresh\"].mean()/3.)\n",
    "print(R_per_sample, R_per_protein, R_per_protein_families, accuracy, R_per_sample_thresh_aleatoric, R_per_protein_thresh_aleatoric, R_per_protein_families_thresh_aleatoric)\n",
    "\n",
    "\n",
    "with open(inf_type+\"_metrics_recap.txt\", \"w\") as file:\n",
    "    \n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error per samples = \" + str(R_per_sample) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error per protein = \" + str(R_per_protein) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error per protein families = \" + str(R_per_protein_families) + \"\\n\")\n",
    "\n",
    "    file.write(\"accuracy = \" + str(accuracy) + \"\\n\")\n",
    "\n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error (thresholded) per samples = \" + str(R_per_sample_thresh) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error (thresholded) per protein = \" + str(R_per_protein_thresh) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) epistemic uncertainty to error (thresholded) per protein families = \" + str(R_per_protein_families_thresh) + \"\\n\")\n",
    "\n",
    "    file.write(\"pearson correlation (R) aleatoric uncertainty to error (thresholded) per samples = \" + str(R_per_sample_thresh_aleatoric) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) aleatoric uncertainty to error (thresholded) per protein = \" + str(R_per_protein_thresh_aleatoric) + \"\\n\")\n",
    "    file.write(\"pearson correlation (R) aleatoric uncertainty to error (thresholded) per protein families = \" + str(R_per_protein_families_thresh_aleatoric) + \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('loic_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bdf57c6960395b8f864a402d628317266c30e42f194c32f5c9b81ad3912df87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
